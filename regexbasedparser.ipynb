{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1rN5t3OR/9cFHDJw8KKPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yampss/ML-projects/blob/main/regexbasedparser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "\n",
        "!pip install docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USL4GL1OF_M4",
        "outputId": "76035970-b814-4797-db5b-6f5e242f56ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/232.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: docx in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vci25QwHD8W",
        "outputId": "30e8a061-eb70-4c00-f50f-f1801cf06e40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/239.6 kB\u001b[0m \u001b[31m776.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/239.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48mieIpjQXrR",
        "outputId": "91d33e0e-2c1f-4b50-c5f5-e562b6ff8751"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import PyPDF2\n",
        "import docx\n",
        "import string\n",
        "\n",
        "#Extract text from PDF\n",
        "def getPDFContent(path):\n",
        "    content = \"\"\n",
        "    # Load PDF into pyPDF\n",
        "    with open(path, \"rb\") as file:\n",
        "      pdf = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Iterate pages\n",
        "    # for i in range(0, pdf.getNumPages()):\n",
        "    #     # Extract text from page and add to content\n",
        "    #     content += pdf.getPage(i).extractText() + \"\\n\"\n",
        "      for page in pdf.pages:\n",
        "        content += page.extract_text() + \"\\n\"\n",
        "\n",
        "    # Collapse whitespace\n",
        "    content = \" \".join(content.replace(u\"\\xa0\", \" \").strip().split())\n",
        "    return content\n",
        "\n",
        "#Extract text from DOCX\n",
        "def getText(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    fullText = \"\"\n",
        "    for para in doc.paragraphs:\n",
        "        fullText += para.text\n",
        "    return fullText\n",
        "\n",
        "#To store extracted resumes\n",
        "resume = \"\"\n",
        "\n",
        "filename = input(\"Enter file name / path : \")\n",
        "#Invoking document parsers based on file format\n",
        "#Note: for TXT - do a normal f.read()\n",
        "if filename.endswith(\".pdf\"):\n",
        "    resume = getPDFContent(filename).encode(\"ascii\", \"ignore\")\n",
        "elif filename.endswith(\".docx\"):\n",
        "     resume = getText(filename).encode(\"ascii\", \"ignore\")\n",
        "else:\n",
        "    print(\"File format is currently not supported\")\n",
        "    exit(0)\n",
        "\n",
        "print(\"processing..... \\nplease wait....\")\n",
        "#Importing NLTK for stopword removal and tokenizing\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Tokenizing/ Filtering the resume off stopwords and punctuations\n",
        "print(\"tokenizing the given file ......\")\n",
        "# tokens = word_tokenize(resume)\n",
        "tokens = word_tokenize(resume.decode(\"utf-8\"))\n",
        "\n",
        "punctuations = ['(',')',';',':','[',']',',']\n",
        "stop_words = stopwords.words('english')\n",
        "#storing the cleaned resume\n",
        "filtered = [w for w in tokens if not w in stop_words and  not w in string.punctuation]\n",
        "print(\"removing the stop words....\\nCleaning the resumes....\\nExtracting Text .......\")\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "# print(filtered)\n",
        "#get the name from the resume\n",
        "name  = str(filtered[0])+' ' +str(filtered[1])\n",
        "print(\"Name : \" + name)\n",
        "\n",
        "#using regular expressions we extract phone numbers and mail ids\n",
        "import re\n",
        "#get contact info - from resume\n",
        "#email\n",
        "email = \"\"\n",
        "# match_mail = re.search(r'[\\w\\.-]+@[\\w\\.-]+', resume)\n",
        "match_mail = re.search(r'[\\w\\.-]+@[\\w\\.-]+', resume.decode(\"utf-8\"))\n",
        "\n",
        "#handling the cases when mobile number is not given\n",
        "if(match_mail != None):\n",
        "    email = match_mail.group(0)\n",
        "print(\"Email : \" + email)\n",
        "\n",
        "#mobile number\n",
        "mobile = \"\"\n",
        "# match_mobile = re.search(r'((?:\\(?\\+91\\)?)?\\d{9})',resume)\n",
        "match_mobile = re.search(r'((?:\\(?\\+91\\)?)?\\d{9})', resume.decode(\"utf-8\"))\n",
        "\n",
        "#handling the cases when mobile number is not given\n",
        "if(match_mobile != None):\n",
        "    mobile = match_mobile.group(0)\n",
        "print(\"Mobile : \" +  mobile)\n",
        "\n",
        "parsed_resume = ' '.join(filtered)\n",
        "# print(\"Parsed Resume in plain Text : \", parsed_resume)\n",
        "# print(\"Parsed Resume in plain Text:\")\n",
        "# words_per_line = 10\n",
        "# for i in range(0, len(filtered), words_per_line):\n",
        "#     line = ' '.join(filtered[i:i+words_per_line])\n",
        "#     print(line)\n",
        "# Example string\n",
        "string_data = parsed_resume\n",
        "\n",
        "# Split the string into a list using a specific delimiter (comma in this case)\n",
        "elements_list = string_data.split(' ')\n",
        "\n",
        "# Convert the list to a tuple\n",
        "tuple_data = tuple(elements_list)\n",
        "\n",
        "# Print the result\n",
        "print(tuple_data)\n"
      ],
      "metadata": {
        "id": "0P7LPbFIAyXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c2bb83-d239-4704-81b5-b274d7cb19ac"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter file name / path : /content/Sample_Resume.pdf\n",
            "processing..... \n",
            "please wait....\n",
            "tokenizing the given file ......\n",
            "removing the stop words....\n",
            "Cleaning the resumes....\n",
            "Extracting Text .......\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Name : ANUVA GOYAL\n",
            "Email : anuvagoyal111@gmail.com\n",
            "Mobile : 952034954\n",
            "('ANUVA', 'GOYAL', 'D.O.B', '1st', 'October', '2000', 'Gender', 'Female', 'OBJECTIVE', 'Energetic', 'innovative', 'engineering', 'undergraduate', 'passionate', 'Machine', 'Learning', 'NLP', 'Deep', 'Learning', 'solving', 'real', '-world', 'problems', 'aiming', 'work', 'organization', 'providing', 'great', 'learning', 'experience', 'growth', 'opportunities', 'mutual', 'benefit', 'EDUCATION', 'Qualification', 'Institute', 'CGPA', 'Year', 'Completion', 'B.Tech', 'Dayalbagh', 'Educational', 'Institute', 'CGPA', '9.35', '2023', 'Electrical', 'Engineering', 'Till', '4', 'Dayalbagh', 'Agra', 'Pursuing', 'Specialization', 'Computer', 'Science', 'semesters', 'XII', 'St.', 'Clares', 'Senior', 'Secondary', 'School', 'Agra', '94', '2019', 'X', 'St.', 'Clares', 'Senior', 'Secondary', 'School', 'Agra', 'CGPA', '10', '2017', 'INTERNSHIPS', 'AND', 'TRAININGS', 'Internships', 'Completed', 'internship', 'Genisup', 'India', 'Pvt', 'Ltd.', 'July', '2021', 'Aug', '2021', 'NLP', 'Topic', 'Modeling', 'Worked', 'proxy', 'rotation', 'utilization', 'Learnt', 'Web', 'Scraping', 'using', 'bs4', 'performed', 'LDA', 'Topic', 'Modeling', 'The', 'Hindu', 'news', 'articles', 'Worked', 'intern', 'VUGS', 'Technologies', 'May', '2021', 'June', '2021', 'Built', 'OCR', 'using', 'Pytesseract', 'NER', 'Text', 'Classification', 'Model', 'categorize', 'detected', 'text', 'Name', 'E', '-mail', 'Address', 'Phone', 'number', 'Date', 'Trainings', 'Undertaking', 'Deep', 'learning', 'Computer', 'Vision', 'A', '-Z', 'OpenCV', 'SSD', 'GANs', 'course', 'f', 'rom', 'Udemy', 'Nov', '2021', 'Completed', 'Neural', 'Networks', 'Deep', 'Learning', 'course', 'Coursera', 'Apr', '2021', 'Undertook', 'Machine', 'Learning', 'A', '-Z', 'Hands', 'Python', 'R', 'Data', 'Science', 'course', 'Udemy', 'Feb', '2021', 'Completed', 'Python', 'Core', 'course', 'SoloLearn', 'Dec', '2020', 'PROJECTS', 'Currently', 'working', 'Mental', 'Healthcare', 'Chatbot', 'Ongoing', 'Built', 'full', 'stack', 'Speech', 'Emotion', 'based', 'Movie', 'Recommender', 'system', 'using', 'RAVDESS', 'Dataset', 'SER', 'model', 'Web', 'Scraping', 'techniques', 'recommendation', 'system', 'Oct', '2021', 'Finding', 'Perfect', 'Fit', 'model', 'parse', 'resumes', 'using', 'Pytesseract', 'extract', 'text', 'NLP', 'data', 'wrangling', 'XG', 'Boost', 'Random', 'Forest', 'technique', 'classification', 'Aug', '2021', 'OCR', 'Handwritten', 'text', 'model', 'digit', '0', '-9', 'A', '-Z', 'characters', 'classification', 'using', 'CNN', 'architecture', 'detect', 'final', 'word', 'image', 'June', '2021', 'Face', 'Detection', 'Recognition', 'using', 'OpenCV', 'TensorFlow', 'May', '20', '21', 'Face', 'Mask', 'Detection', 'detects', 'face', 'using', 'Haar', 'Cascade', 'Classifier', 'OpenCV', 'library', 'classifies', 'image', 'one', 'three', 'categories', 'Without', 'Mask', 'With', 'Mask', 'Incorrect', 'Mask', 'help', 'LBPH', 'Algorithm', 'May', '2021', 'Heart', 'Attack', 'Prediction', 'Insurance', 'Company', 'Prediction', 'using', 'various', 'classification', 'algorithms', 'SKILLS', 'Experience', 'C', 'C++', 'Python', 'JAVA', 'HTML', 'CSS', 'JavaScript', 'Data', 'Structures', 'SQL', 'Softwares', 'PyCharm', 'Jupyter', 'Notebook', 'Google', 'Colab', 'Code', 'Blocks', 'MATLAB', 'Turbo', 'C++', 'MS', 'Office', 'Machine', 'Learning', 'Frameworks', 'Scikit', '-Learn', 'TensorFlow', 'OpenCV', 'NumPy', 'Pytesseract', 'Keras', 'ACHIEVEMENTS', 'AND', 'CERTIFICATIONS', 'Secured', '3rd', 'position', 'TECH', '-A-THON', 'organized', 'The', 'ECE', 'Society', 'BIT', 'Mesra', 'Ranchi', 'Oct', '2021', 'Participant', '30', 'Days', 'Google', 'Cloud', 'Sep', '2021', 'Oct', '2021', 'Won', '1st', 'prize', 'online', 'competition', 'Game', 'Brands', 'organized', 'SGGSCC', 'University', 'Delhi', 'Mar', '2021', 'Secured', '2nd', 'rank', 'Street', 'Play', 'Intra', 'Faculty', 'Competition', 'theme', 'Women', 'Empowerment', 'Sep', '2021', 'WORKSHOPS', 'AND', 'EVENTS', 'Finalist', 'Rise', 'Crisis', 'crisis', 'management', 'competition', 'organized', 'SRCC', 'University', 'Delhi', 'Nov', '2021', 'Participated', 'Hero', 'Campus', 'C', 'hallenge', 'S7', 'national', 'level', 'competition', 'organized', 'Hero', 'MotoCorp', 'Limited', 'Nov', '2020', 'Attended', 'online', 'KLA', 'Workshop', 'AI', 'HPC', 'Semiconductor', 'Manufacturing', 'organized', 'IIT', 'Madras', 'Sep', '2021', 'Actively', 'participated', 'NLP', 'Disaster', 'Tweets', 'organized', 'Kaggle', 'Aug', '2021', 'Qualified', 'Vishleshan', 'The', 'Analytics', 'Event', 'organized', 'NIT', 'Trichy', 'Nov', '2020', 'Competed', 'Prabandhan20', 'Annual', 'Management', 'Conclave', 'organized', 'IIT', 'Kanpur', 'Sep', '2020', 'Participated', '5', 'days', 'course', 'Geospatial', 'Inputs', 'Enabling', 'Master', 'Plan', 'Formulation', 'conducted', 'Indian', 'Institute', 'Remote', 'Sensing', 'July', '2020', 'Member', 'Stage', 'Management', 'Team', 'Drama', 'Fest', 'District', 'level', 'Feb', '2020', 'Served', 'member', 'National', 'S', 'ervice', 'Scheme', '7', 'days', 'NSS', 'Camp', 'Jan', '2020', 'INTERESTS', 'AND', 'HOBBIES', 'Machine', 'Learning', 'Data', 'Science', 'Reading', 'novels', 'Volunteering', 'social', 'service', 'Phone', '+91', '9520349542', 'Email', 'anuvagoyal111', 'gmail.com', 'GitHub', 'https', '//github.com/AnuvaGoyal')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwacCgm5FdD-"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}